{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dcai.score import ScoreTracker\n",
    "from dcai.dataset import TrainDataset, ValidationDataset\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start a score tracker to track score as function of the number of annotations bought\n",
    "score_tracker = ScoreTracker(team_name=\"marcel-brute-force\")\n",
    "\n",
    "# Get a MNIST train data set\n",
    "train_dataset = TrainDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of buying an annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(2,2))\n",
    "plt.title(f\"Given label = {train_dataset[15][1]}\")\n",
    "plt.imshow(train_dataset[15][0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Buy an annotation\n",
    "train_dataset.buy_annotation(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(2,2))\n",
    "plt.title(f\"Given label = {train_dataset[15][1]}\")\n",
    "plt.imshow(train_dataset[15][0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for x in [1,2,7]:\n",
    "    print(f\"Num examples for given class {x}: {np.sum(train_dataset.y == x)}\")\n",
    "    # np.sum(train_dataset.y == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with the current data set and get the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = score_tracker.train_and_score_model(train_dataset, plot_confusion_matrix=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score function returns the fitted model, so you can use this model for active learning etc. You don't have to use this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What else can you do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(train_dataset.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Naive/baseline approach: Randomly annotate!\n",
    "\n",
    "Your method should at least have a better performance than this :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def iterate_in_batches(it: List, batch_size: int):\n",
    "    i=0\n",
    "    while True:\n",
    "        ret = it[i:i+batch_size]\n",
    "        if len(ret) == 0:\n",
    "            return\n",
    "        yield ret\n",
    "        i += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = pd.Series(train_dataset.y)\n",
    "labels_1 = labels[labels == 1]\n",
    "\n",
    "# Excluding the given label class 1, we will only include data points that we have bought annotations for\n",
    "train_dataset.exclude_datapoints(labels_1.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for batch in iterate_in_batches(labels_1, 2048):\n",
    "    for el in batch.index:\n",
    "        train_dataset.buy_annotation(el)\n",
    "    train_dataset.include_datapoints(batch.index)\n",
    "        \n",
    "    score_tracker.train_and_score_model(train_dataset, plot_confusion_matrix=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_tracker.train_and_score_model(train_dataset, plot_confusion_matrix=True)\n",
    "score_tracker.plot_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
